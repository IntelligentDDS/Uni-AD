{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import transformer_ad\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "#tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift(arr, num, fill_value=np.nan):\n",
    "    arr = np.roll(arr, num)\n",
    "    if num < 0:\n",
    "        arr[num:] = fill_value\n",
    "    elif num > 0:\n",
    "        arr[:num] = fill_value\n",
    "    return arr\n",
    "def filter_pred(values_pred, scale=3):\n",
    "    predicted_anomalies_ = np.argwhere(values_pred == 1).ravel()\n",
    "    predicted_anomalies_shift_forward = shift(predicted_anomalies_, 1, fill_value=predicted_anomalies_[0])\n",
    "    predicted_anomalies_shift_backward = shift(predicted_anomalies_, -1, fill_value=predicted_anomalies_[-1])\n",
    "    predicted_anomalies_start = np.argwhere(\n",
    "        (predicted_anomalies_shift_forward - predicted_anomalies_) != -1\n",
    "    ).ravel()\n",
    "    predicted_anomalies_finish = np.argwhere(\n",
    "        (predicted_anomalies_ - predicted_anomalies_shift_backward) != -1\n",
    "    ).ravel()\n",
    "    predicted_anomalies = np.hstack(\n",
    "        [\n",
    "            predicted_anomalies_[predicted_anomalies_start].reshape(-1, 1),\n",
    "            predicted_anomalies_[predicted_anomalies_finish].reshape(-1, 1),\n",
    "        ]\n",
    "    )\n",
    "    for a_range in predicted_anomalies:\n",
    "        if a_range[1]-a_range[0]<=scale-1:\n",
    "            values_pred[a_range[0]:a_range[1]+1] = 0\n",
    "    return values_pred\n",
    "\n",
    "from prts import ts_precision, ts_recall, ts_fscore\n",
    "def bf_search(label, score, verbose=True, is_filter=False):\n",
    "    \"\"\"\n",
    "    Find the best-f1 score by searching best `threshold` in [`start`, `end`).\n",
    "    Returns:\n",
    "        list: list for results\n",
    "        float: the `threshold` for best-f1\n",
    "    \"\"\"\n",
    "    start = 90\n",
    "    search_range = [np.percentile(score, q) for q in np.arange(start, 100, 0.1)]\n",
    "    m = {'f1-score':-1., 'precision':-1., 'recall':-1.}\n",
    "    m_t = 0.0\n",
    "    #print(len(score))\n",
    "    #print(len(search_range))\n",
    "    for threshold in sorted(search_range)[::-1]:\n",
    "        real = label\n",
    "        pred = score > threshold\n",
    "        #print(np.unique(pred))\n",
    "        if is_filter:\n",
    "            pred = filter_pred(pred, scale=1)\n",
    "        #pred = filter_pred(pred, scale=3)\n",
    "        if len(np.unique(pred))==1:\n",
    "            continue\n",
    "        target = ts_fscore(real, pred, beta=1.0, p_alpha=0.0, r_alpha=0.5, cardinality=\"reciprocal\", p_bias=\"front\", r_bias=\"front\")\n",
    "        if target > m['f1-score']:\n",
    "            m_t = threshold\n",
    "            m['f1-score'] = target\n",
    "            m['precision'] = ts_precision(real, pred, alpha=0.0, cardinality=\"reciprocal\", bias=\"front\")\n",
    "            m['recall'] = ts_recall(real, pred, alpha=0.5, cardinality=\"reciprocal\", bias=\"front\")\n",
    "            if verbose:\n",
    "                print(\"cur thr: \", threshold, target, m, m_t)\n",
    "    #print(m, m_t)\n",
    "    return m, m_t\n",
    "\n",
    "def calc_point2point(predict, actual):\n",
    "    \"\"\"\n",
    "    calculate f1 score by predict and actual.\n",
    "\n",
    "    Args:\n",
    "        predict (np.ndarray): the predict label\n",
    "        actual (np.ndarray): np.ndarray\n",
    "    \"\"\"\n",
    "    TP = np.sum(predict * actual)\n",
    "    TN = np.sum((1 - predict) * (1 - actual))\n",
    "    FP = np.sum(predict * (1 - actual))\n",
    "    FN = np.sum((1 - predict) * actual)\n",
    "    precision = TP / (TP + FP + 0.00001)\n",
    "    recall = TP / (TP + FN + 0.00001)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 0.00001)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def adjust_predicts(score, label,\n",
    "                    threshold=None,\n",
    "                    pred=None,\n",
    "                    calc_latency=False,\n",
    "                    is_filter=False):\n",
    "    \"\"\"\n",
    "    Calculate adjusted predict labels using given `score`, `threshold` (or given `pred`) and `label`.\n",
    "\n",
    "    Args:\n",
    "        score (np.ndarray): The anomaly score\n",
    "        label (np.ndarray): The ground-truth label\n",
    "        threshold (float): The threshold of anomaly score.\n",
    "            A point is labeled as \"anomaly\" if its score is lower than the threshold.\n",
    "        pred (np.ndarray or None): if not None, adjust `pred` and ignore `score` and `threshold`,\n",
    "        calc_latency (bool):\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: predict labels\n",
    "    \"\"\"\n",
    "    if len(score) != len(label):\n",
    "        raise ValueError(\"score and label must have the same length\")\n",
    "    score = np.asarray(score)\n",
    "    label = np.asarray(label)\n",
    "    latency = 0\n",
    "    if pred is None:\n",
    "        predict = score > threshold\n",
    "        if is_filter:\n",
    "            predict = filter_pred(predict, scale=1)\n",
    "    else:\n",
    "        predict = pred\n",
    "    actual = label > 0.1\n",
    "    anomaly_state = False\n",
    "    anomaly_count = 0\n",
    "    for i in range(len(score)):\n",
    "        if actual[i] and predict[i] and not anomaly_state:\n",
    "            anomaly_state = True\n",
    "            anomaly_count += 1\n",
    "            for j in range(i, 0, -1):\n",
    "                if not actual[j]:\n",
    "                    break\n",
    "                else:\n",
    "                    if not predict[j]:\n",
    "                        predict[j] = True\n",
    "                        latency += 1\n",
    "        elif not actual[i]:\n",
    "            anomaly_state = False\n",
    "        if anomaly_state:\n",
    "            predict[i] = True\n",
    "    if calc_latency:\n",
    "        return predict, latency / (anomaly_count + 1e-4)\n",
    "    else:\n",
    "        return predict\n",
    "\n",
    "def bf_search_omni(label, score, verbose=True, is_filter=False):\n",
    "    \"\"\"\n",
    "    Find the best-f1 score by searching best `threshold` in [`start`, `end`).\n",
    "    Returns:\n",
    "        list: list for results\n",
    "        float: the `threshold` for best-f1\n",
    "    \"\"\"\n",
    "    start = 90\n",
    "    search_range = [np.percentile(score, q) for q in np.arange(start, 100, 0.1)]\n",
    "    m = {'f1-score':-1., 'precision':-1., 'recall':-1.}\n",
    "    m_t = 0.0\n",
    "    #print(len(score))\n",
    "    #print(len(search_range))\n",
    "    for threshold in sorted(search_range)[::-1]:\n",
    "        real = label\n",
    "        pred = adjust_predicts(score, label, threshold, is_filter=is_filter)\n",
    "        #print(np.unique(pred))\n",
    "        if len(np.unique(pred))==1:\n",
    "            continue\n",
    "        target = calc_point2point(pred, label)\n",
    "        if target[0] > m['f1-score']:\n",
    "            m_t = threshold\n",
    "            m['f1-score'] = target[0]\n",
    "            m['precision'] = target[1]\n",
    "            m['recall'] = target[2]\n",
    "            if verbose:\n",
    "                print(\"cur thr: \", threshold, target, m, m_t)\n",
    "    #print(m, m_t)\n",
    "    return m, m_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_rangebased(label, score, is_filter=False):\n",
    "    \"\"\"\n",
    "    Find the best-f1 score by searching best `threshold` in [`start`, `end`).\n",
    "    Returns:\n",
    "        list: list for results\n",
    "        float: the `threshold` for best-f1\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    search_range = [np.percentile(score, q) for q in np.arange(start, 100, 0.1)]\n",
    "    m = {}\n",
    "    m['precision'] = []\n",
    "    m['recall'] = []\n",
    "    for threshold in sorted(search_range):\n",
    "        real = label\n",
    "        pred = score > threshold\n",
    "        #print(np.unique(pred))\n",
    "        if is_filter:\n",
    "            pred = filter_pred(pred, scale=1)\n",
    "        #pred = filter_pred(pred, scale=3)\n",
    "        if len(np.unique(pred))==1:\n",
    "            continue\n",
    "        m['precision'].append(ts_precision(real, pred, alpha=0.0, cardinality=\"reciprocal\", bias=\"front\"))\n",
    "        m['recall'].append(ts_recall(real, pred, alpha=0.5, cardinality=\"reciprocal\", bias=\"front\"))\n",
    "    # The last precision and recall values are 1. and 0\n",
    "    m['precision'].append(1)\n",
    "    m['recall'].append(0)\n",
    "    ap = -np.sum(np.diff(m['recall']) * np.array(m['precision'])[1:])\n",
    "    #print(m, ap)\n",
    "    #print(ap)\n",
    "    return ap\n",
    "\n",
    "def ap_pointadjust(label, score, is_filter=False):\n",
    "    \"\"\"\n",
    "    Find the best-f1 score by searching best `threshold` in [`start`, `end`).\n",
    "    Returns:\n",
    "        list: list for results\n",
    "        float: the `threshold` for best-f1\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    search_range = [np.percentile(score, q) for q in np.arange(start, 100, 0.1)]\n",
    "    m = {}\n",
    "    m['precision'] = []\n",
    "    m['recall'] = []\n",
    "    for threshold in sorted(search_range):\n",
    "        real = label\n",
    "        pred = adjust_predicts(score, label, threshold, is_filter=is_filter)\n",
    "        #print(np.unique(pred))\n",
    "        if len(np.unique(pred))==1:\n",
    "            continue\n",
    "        target = calc_point2point(pred, label)\n",
    "        m['precision'].append(target[1])\n",
    "        m['recall'].append(target[2])\n",
    "    # The last precision and recall values are 1. and 0\n",
    "    m['precision'].append(1)\n",
    "    m['recall'].append(0)\n",
    "    ap = -np.sum(np.diff(m['recall']) * np.array(m['precision'])[1:])\n",
    "    #print(m, ap)\n",
    "    #print(ap)\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles = sorted(os.listdir('CTF_data/CTF_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles = [f for f in datafiles if '118' not in f and '166' not in f and '174' not in f and '263' not in f and '289' not in f and '307' not in f and '419' not in f and '432' not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datafiles)/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name='iterate-huber-nosfa-nolongterm-ep30-1e-4-noisy'\n",
    "lr=1e-4\n",
    "num_epochs=5\n",
    "sequence_length = 100\n",
    "z_dim = 16\n",
    "batch_size = 128\n",
    "gpu_choice = 3\n",
    "use_sfa = False\n",
    "no_longterm = True\n",
    "no_featerm = False\n",
    "noisy_rate = 0\n",
    "loss_func = 'huber'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'transformer_ad'\n",
    "name += '-'\n",
    "name += loss_func\n",
    "name += '-'\n",
    "if use_sfa:\n",
    "    name += 'usesfa'\n",
    "    name += '-'\n",
    "else:\n",
    "    name += 'nosfa'\n",
    "    name += '-'\n",
    "if no_longterm:\n",
    "    name += 'nolongterm'\n",
    "    name += '-'\n",
    "else:\n",
    "    name += 'uselongterm'\n",
    "    name += '-'\n",
    "if no_featerm:\n",
    "    name += 'nofeaterm'\n",
    "    name += '-'\n",
    "else:\n",
    "    name += 'usefeaterm'\n",
    "    name += '-'\n",
    "name += 'ep'\n",
    "name += str(num_epochs)\n",
    "name += '-'\n",
    "name += 'z'\n",
    "name += str(z_dim)\n",
    "name += '-'\n",
    "if noisy_rate != 0:\n",
    "    name += 'noisy'\n",
    "    name += str(noisy_rate)\n",
    "    name += '-'\n",
    "else:\n",
    "    name += 'nonoisy'\n",
    "basename = name\n",
    "name += time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime()) \n",
    "print(basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = transformer_ad.MonitorEntityDataset(datafiles, sequence_length, z_dim, gpu=gpu_choice, use_sfa=use_sfa, no_longterm=no_longterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_range = [1e-4, 1e-3]\n",
    "hid_range = [3, 8, 16]\n",
    "\n",
    "point_adjust_f1_LIST = []\n",
    "range_based_f1_LIST = []\n",
    "\n",
    "point_adjust_ap_LIST = []\n",
    "range_based_ap_LIST = []\n",
    "\n",
    "#with open(basename+'_transformerad_experiment_results.txt', 'a') as f:\n",
    "#    f.write(name+'\\n')\n",
    "with open('transformerad_experiment_results_CTFDATA.txt', 'a') as f:\n",
    "    f.write(name+'\\n')\n",
    "for lr_example in lr_range:\n",
    "    for hid_example in hid_range:\n",
    "        name = 'transformer_ad'\n",
    "        name += '-'\n",
    "        name += loss_func\n",
    "        name += '-'\n",
    "        if use_sfa:\n",
    "            name += 'usesfa'\n",
    "            name += '-'\n",
    "        else:\n",
    "            name += 'nosfa'\n",
    "            name += '-'\n",
    "        if no_longterm:\n",
    "            name += 'nolongterm'\n",
    "            name += '-'\n",
    "        else:\n",
    "            name += 'uselongterm'\n",
    "            name += '-'\n",
    "        if no_featerm:\n",
    "            name += 'nofeaterm'\n",
    "            name += '-'\n",
    "        else:\n",
    "            name += 'usefeaterm'\n",
    "            name += '-'\n",
    "        name += 'ep'\n",
    "        name += str(num_epochs)\n",
    "        name += '-'\n",
    "        name += 'z'\n",
    "        name += '-'\n",
    "        if noisy_rate != 0:\n",
    "            name += 'noisy'\n",
    "            name += str(noisy_rate)\n",
    "            name += '-'\n",
    "        else:\n",
    "            name += 'nonoisy'\n",
    "        basename = name\n",
    "        name += time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime()) \n",
    "        print(name)\n",
    "        \n",
    "        \n",
    "        dataset.train()\n",
    "        model = transformer_ad.Transformer_AD(name=name, lr=lr_example, num_epochs=num_epochs, hidden_dim=hid_example, batch_size=batch_size, gpu=gpu_choice, use_sfa=use_sfa, no_longterm=no_longterm, no_featerm=no_featerm, noisy_rate=noisy_rate)\n",
    "        model.fit(dataset, loss_func=loss_func, log_step=60)\n",
    "        \n",
    "        mac_ids = sorted(list(set([f.split('_')[0] for f in datafiles])))\n",
    "        aucs = []\n",
    "        prfs = {}\n",
    "        prfs_omni = {}\n",
    "        \n",
    "        aps_rbased = {}\n",
    "        aps_padjust = {}\n",
    "        gc.collect()\n",
    "        for i in range(len(mac_ids)):\n",
    "            print(mac_ids[i])\n",
    "\n",
    "            dataset.test(mac_ids[i])\n",
    "            with open('CTF_data/label_result/' + mac_ids[i]+'.pkl', 'rb') as label_file:\n",
    "                label = pk.load(label_file)\n",
    "\n",
    "            pred = model.predict(dataset)\n",
    "            pred = np.mean(pred, axis = 2)[1:,-1]\n",
    "\n",
    "            aucs.append(roc_auc_score(label, pred))\n",
    "    \n",
    "            prfs[mac_ids[i]] = bf_search(label.flatten(), pred, verbose=False)\n",
    "\n",
    "            prfs_omni[mac_ids[i]] = bf_search_omni(label.flatten(), pred, verbose=False)\n",
    "            \n",
    "            aps_rbased[mac_ids[i]] = ap_rangebased(label, pred)\n",
    "            aps_padjust[mac_ids[i]] = ap_pointadjust(label, pred)\n",
    "            \n",
    "        omnifscore = [item[0]['f1-score'] for item in list(prfs_omni.values())]\n",
    "        point_adjust_f1 = np.mean(omnifscore)\n",
    "\n",
    "        rangefscore = [item[0]['f1-score'] for item in list(prfs.values())]\n",
    "        range_based_f1 = np.mean(rangefscore)\n",
    "        \n",
    "        point_adjust_ap = np.mean(list(aps_padjust.values()))\n",
    "        range_based_ap = np.mean(list(aps_rbased.values()))\n",
    "        \n",
    "        point_adjust_f1_LIST.append(point_adjust_f1)\n",
    "        range_based_f1_LIST.append(range_based_f1)\n",
    "\n",
    "        point_adjust_ap_LIST.append(point_adjust_ap)\n",
    "        range_based_ap_LIST.append(range_based_ap)\n",
    "\n",
    "        with open('transformerad_experiment_results_CTFDATA.txt', 'a') as f:\n",
    "            f.write('lr: '+str(lr_example)+', hid: '+str(hid_example)+', point_adjust_f1: '+str(point_adjust_f1)+', range_based_f1: '+str(range_based_f1)+', point_adjust_ap: '+str(point_adjust_ap)+', range_based_ap: '+str(range_based_ap)+'\\n')\n",
    "            \n",
    "with open('transformerad_experiment_results_CTFDATA.txt', 'a') as f:\n",
    "    f.write(name+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('point_adjust_f1_LIST =', point_adjust_f1_LIST)\n",
    "print('range_based_f1_LIST =', range_based_f1_LIST)\n",
    "\n",
    "print('point_adjust_ap_LIST =', point_adjust_ap_LIST)\n",
    "print('range_based_ap_LIST =', range_based_ap_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch1.7)",
   "language": "python",
   "name": "torch1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
